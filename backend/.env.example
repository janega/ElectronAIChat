# =============================================================================
# LLM PROVIDER CONFIGURATION
# Choose "ollama" for local models, "openai" for OpenAI API, or "llamacpp" for GGUF models
# =============================================================================
LLM_PROVIDER=ollama

# -----------------------------------------------------------------------------
# OLLAMA SETTINGS (for local LLM hosting)
# -----------------------------------------------------------------------------
OLLAMA_HOST=http://localhost:11434
OLLAMA_EMBED_MODEL=nomic-embed-text
OLLAMA_LLM_MODEL=llama3:latest

# -----------------------------------------------------------------------------
# OPENAI SETTINGS (for OpenAI API)
# -----------------------------------------------------------------------------
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_EMBED_MODEL=text-embedding-3-small
OPENAI_LLM_MODEL=gpt-3.5-turbo

# =============================================================================
# APPLICATION SETTINGS
# =============================================================================
APP_NAME=ElectronAIChat
USE_APP_DATA_DIR=false
BASE_DIR=.
LOG_LEVEL=INFO

# =============================================================================
# FILE UPLOAD SETTINGS
# =============================================================================
MAX_UPLOAD_SIZE_MB=100

# -----------------------------------------------------------------------------
# LLAMACPP SETTINGS (for local GGUF model inference)
# -----------------------------------------------------------------------------
# LLM_PROVIDER=llamacpp

# Model paths (relative to BASE_DIR or absolute)
LLAMACPP_MODELS_DIR=./models
LLAMACPP_CHAT_MODEL=qwen3-0.6b-q4.gguf
LLAMACPP_EMBED_MODEL=nomic-embed-text-q4.gguf

# Performance settings
LLAMACPP_ENABLE_PARALLEL=false
LLAMACPP_N_CTX=2048
LLAMACPP_VERBOSE=false

# GPU settings (optional, defaults to auto-detect)
# LLAMACPP_N_GPU_LAYERS=-1  # -1 for all layers, 0 for CPU-only, or specific number

# OCR Configuration
# Path to Poppler binaries for PDF processing
# Windows example: D:\path\to\poppler-xx.xx.x\Library\bin
# Linux example: /usr/bin
# macOS example: /usr/local/bin
POPPLER_PATH=


# =============================================================================
# ADVANCED SETTINGS (Not Yet Implemented - Hardcoded Defaults)
# =============================================================================
# These settings are NOT currently supported but may be added in future versions:
#
# Text chunking (currently hardcoded: chunk_size=500, overlap=50):
#   EMBED_CHUNK_SIZE=500
#   EMBED_CHUNK_OVERLAP=50
#
# Server configuration (currently hardcoded: host=127.0.0.1, port=8000):
#   HOST=127.0.0.1
#   PORT=8000
#   RELOAD=false
#
# Memory configuration (not implemented):
#   MEM0_CONFIG_PATH=mem0_config.json
#
# Note: ChromaDB path is automatically set to {BASE_DIR}/chroma_db
