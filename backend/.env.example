# =============================================================================
# LLM PROVIDER CONFIGURATION
# Choose "auto" (auto-detect), "ollama", "llamacpp", or "openai"
# - "auto" will detect in order: ollama → llamacpp → openai
# - Set explicitly to force a specific provider
# =============================================================================
LLM_PROVIDER=auto

# -----------------------------------------------------------------------------
# OLLAMA SETTINGS (for local LLM hosting)
# -----------------------------------------------------------------------------
OLLAMA_HOST=http://localhost:11434
OLLAMA_EMBED_MODEL=nomic-embed-text:latest
OLLAMA_LLM_MODEL=llama2:latest

# -----------------------------------------------------------------------------
# OPENAI SETTINGS (for OpenAI API)
# -----------------------------------------------------------------------------
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_EMBED_MODEL=text-embedding-3-small
OPENAI_LLM_MODEL=gpt-3.5-turbo

# =============================================================================
# APPLICATION SETTINGS
# =============================================================================
APP_NAME=ElectronAIChat
USE_APP_DATA_DIR=false
BASE_DIR=.
LOG_LEVEL=INFO

# =============================================================================
# FILE UPLOAD SETTINGS
# =============================================================================
MAX_UPLOAD_SIZE_MB=100

# =============================================================================
# OCR CONFIGURATION
# Path to Poppler binaries for PDF processing with pdf2image
# - Windows: D:\path\to\poppler-xx.xx.x\Library\bin
# - Linux: /usr/bin
# - macOS: /usr/local/bin
# Leave empty if poppler is in system PATH
# =============================================================================
POPPLER_PATH=

# -----------------------------------------------------------------------------
# LLAMACPP SETTINGS (for local GGUF model inference)
# -----------------------------------------------------------------------------
# Uncomment to use llamacpp provider: LLM_PROVIDER=llamacpp

# Model paths (relative to BASE_DIR or absolute)
LLAMACPP_MODELS_DIR=./models
LLAMACPP_CHAT_MODEL=qwen3-0.6b-q4.gguf
LLAMACPP_EMBED_MODEL=nomic-embed-text-q4.gguf

# Performance settings
LLAMACPP_ENABLE_PARALLEL=false
LLAMACPP_N_CTX=2048
LLAMACPP_VERBOSE=false

# GPU settings (optional, defaults to auto-detect)
# LLAMACPP_N_GPU_LAYERS=-1  # -1 for all layers, 0 for CPU-only, or specific number


# =============================================================================
# ADVANCED SETTINGS (Not Yet Implemented - Hardcoded Defaults)
# =============================================================================
# These settings are NOT currently supported but may be added in future versions:
#
# Text chunking (currently hardcoded: chunk_size=500, overlap=50):
#   EMBED_CHUNK_SIZE=500
#   EMBED_CHUNK_OVERLAP=50
#
# Server configuration (currently hardcoded: host=127.0.0.1, port=8000):
#   HOST=127.0.0.1
#   PORT=8000
#   RELOAD=false
#
# Note: ChromaDB path is automatically set to {BASE_DIR}/chroma_db
# Note: Mem0 memory configuration is handled internally
